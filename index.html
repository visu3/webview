<!DOCTYPE html>
<html>
  <head>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision"></script>
    <style>
      body {
        margin: 0;
        overflow: hidden;
        background-color: black;
      }
      video {
        width: 100vw;
        height: 100vh;
        object-fit: cover;
      }
    </style>
  </head>
  <body>
    <video id="webcam" autoplay muted playsinline></video>
    <script type="module">
      import {
        FaceLandmarker,
        FilesetResolver,
      } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest";

      let faceLandmarker;
      let webcam;
      let marStartTime = null;
      const MAR_THRESHOLD = 0.07;
      const MAR_DURATION = 1750; // milliseconds

      function calculateEAR(landmarks) {
        const left = landmarks[159].y - landmarks[145].y;
        const right = landmarks[386].y - landmarks[374].y;
        return (left + right) / 2;
      }

      function calculateMAR(landmarks) {
        const top = landmarks[13].y;
        const bottom = landmarks[14].y;
        return Math.abs(top - bottom);
      }

      async function init() {
        const vision = await FilesetResolver.forVisionTasks(
          "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm"
        );

        faceLandmarker = await FaceLandmarker.createFromOptions(vision, {
          baseOptions: {
            modelAssetPath:
              "https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task",
            delegate: "GPU",
          },
          runningMode: "VIDEO",
          numFaces: 1,
        });

        webcam = document.getElementById("webcam");
        navigator.mediaDevices.getUserMedia({ video: true }).then((stream) => {
          webcam.srcObject = stream;
        });

        detectLoop();
      }

      function detectLoop() {
        const canvas = document.createElement("canvas");
        const ctx = canvas.getContext("2d");

        setInterval(() => {
          if (!faceLandmarker || !webcam.videoWidth) return;

          canvas.width = webcam.videoWidth;
          canvas.height = webcam.videoHeight;
          ctx.drawImage(webcam, 0, 0, canvas.width, canvas.height);
          const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);

          const now = Date.now();
          const results = faceLandmarker.detectForVideo(imageData, now);

          if (results.faceLandmarks.length > 0) {
            const landmarks = results.faceLandmarks[0];
            const ear = calculateEAR(landmarks);
            const mar = calculateMAR(landmarks);

            let status = "Face detected";
            if (mar > MAR_THRESHOLD) {
              if (!marStartTime) marStartTime = now;
              if (now - marStartTime >= MAR_DURATION) {
                status = "Drowsiness Alert";
              }
            } else {
              marStartTime = null;
            }

            const msg = {
              status,
              timestamp: now,
              ear: ear.toFixed(3),
              mar: mar.toFixed(3),
            };

            window.ReactNativeWebView?.postMessage(JSON.stringify(msg));
          } else {
            window.ReactNativeWebView?.postMessage(
              JSON.stringify({ status: "No face detected" })
            );
          }
        }, 1000);
      }

      init();
    </script>
  </body>
</html>
